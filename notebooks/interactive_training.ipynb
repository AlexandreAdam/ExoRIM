{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RIM' from 'ExoRIM.model' (/home/alexandre/Desktop/Projects/ExoRIM/ExoRIM/model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-289196db3949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mExoRIM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCostFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mExoRIM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulated_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCenteredImagesv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mExoRIM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_and_save_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RIM' from 'ExoRIM.model' (/home/alexandre/Desktop/Projects/ExoRIM/ExoRIM/model.py)"
     ]
    }
   ],
   "source": [
    "from ExoRIM.model import RIM, CostFunction\n",
    "from ExoRIM.simulated_data import CenteredImagesv1\n",
    "from ExoRIM.utilities import load_dataset\n",
    "from preprocessing.simulate_data import create_and_save_data\n",
    "import json\n",
    "import numpy as np\n",
    "import os, glob\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.split(os.getcwd())[0]\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id = datetime.now().strftime(\"%y-%m-%d_%H-%M-%S\")\n",
    "#id = \"20-06-21_10-24-04\"\n",
    "#id = '20-06-22_13-38-14'\n",
    "id = '20-06-22_18-19-03'\n",
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(root, \"data\", id)\n",
    "if not os.path.isdir(data_dir): os.mkdir(data_dir)\n",
    "test_dir = os.path.join(root, \"data\", id + \"_test\")\n",
    "if not os.path.isdir(test_dir): os.mkdir(test_dir)\n",
    "projector_dir = os.path.join(root, \"data\", \"projector_arrays\")\n",
    "checkpoint_dir = os.path.join(root, \"models\", id)\n",
    "if not os.path.isdir(checkpoint_dir): os.mkdir(checkpoint_dir)\n",
    "output_dir = os.path.join(root, \"results\", id)\n",
    "if not os.path.isdir(output_dir): os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(os.path.join(root, \"hyperparameters.json\"), \"r\") as f:\n",
    "#    hparams = json.load(f)\n",
    "with open(os.path.join(checkpoint_dir, \"hyperparameters.json\"), \"r\") as f:\n",
    "     hparams = json.load(f)\n",
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "def create_dataset(meta_data, rim, dirname, batch_size=None):\n",
    "    images = tf.convert_to_tensor(create_and_save_data(dirname, meta_data), dtype=tf.float32)\n",
    "    k_images = rim.physical_model.simulate_noisy_image(images)\n",
    "    X = tf.data.Dataset.from_tensor_slices(k_images)  # split along batch dimension\n",
    "    Y = tf.data.Dataset.from_tensor_slices(images)\n",
    "    dataset = tf.data.Dataset.zip((X, Y))\n",
    "    if batch_size is not None: # for train set\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "        dataset = dataset.enumerate(start=0)\n",
    "        dataset = dataset.cache()  # accelerate the second and subsequent iterations over the dataset\n",
    "        dataset = dataset.prefetch(AUTOTUNE)  # Batch is prefetched by CPU while training on the previous batch occurs\n",
    "    else:\n",
    "        # batch together all examples, for test set\n",
    "        dataset = dataset.batch(images.shape[0], drop_remainder=True)\n",
    "        dataset = dataset.cache()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holes = 20\n",
    "# metrics only support grey scale images\n",
    "metrics = {\n",
    "    \"ssim\": lambda Y_pred, Y_true: tf.image.ssim(Y_pred, Y_true, max_val=1.0),\n",
    "    # Bug is tf 2.0.0, make sure filter size is small enough such that H/2**4 and W/2**4 >= filter size\n",
    "    # alternatively (since H/2**4 is = 1 in our case), it is possible to lower the power factors such that\n",
    "    # H/(2**(len(power factor)-1)) > filter size\n",
    "    # Hence, using 3 power factors with filter size=2 works, and so does 2 power factors with filter_size <= 8\n",
    "    # paper power factors are [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]\n",
    "    # After test, it seems filter_size=11 also works with 2 power factors and 32 pixel image\n",
    "    \"ssim_multiscale_01\": lambda Y_pred, Y_true: tf.image.ssim_multiscale(\n",
    "        Y_pred, Y_true, max_val=1.0,\n",
    "        filter_size=11,\n",
    "        power_factors=[0.0448, 0.2856]),\n",
    "    \"ssim_multiscale_23\": lambda Y_pred, Y_true: tf.image.ssim_multiscale(\n",
    "        Y_pred, Y_true, max_val=1.0,\n",
    "        filter_size=11,\n",
    "        power_factors=[0.3001, 0.2363]),\n",
    "    \"ssim_multiscale_34\": lambda Y_pred, Y_true: tf.image.ssim_multiscale(\n",
    "        Y_pred, Y_true, max_val=1.0,\n",
    "        filter_size=11,\n",
    "        power_factors=[0.2363, 0.1333])\n",
    "}\n",
    "cost_function = CostFunction()\n",
    "mask_coordinates = np.loadtxt(os.path.join(projector_dir, f\"mask_{holes}_holes.txt\"))\n",
    "with open(os.path.join(projector_dir, f\"projectors_{holes}_holes.pickle\"), \"rb\") as fb:\n",
    "    arrays = pickle.load(fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight_file = os.path.join(checkpoint_dir, \"rim_005_115.67728.h5\")\n",
    "#weight_file=os.path.join(checkpoint_dir, \"rim_035_11.35379.h5\")\n",
    "weight_file=os.path.join(checkpoint_dir, \"rim_098_15.75180.h5\")\n",
    "#weight_file=None\n",
    "rim = RIM(mask_coordinates=mask_coordinates, hyperparameters=hparams, arrays=arrays, weight_file=weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = CenteredImagesv1(total_items=1000, pixels=32)\n",
    "test_meta = CenteredImagesv1(total_items=200, pixels=32)\n",
    "train_dataset = create_dataset(meta_data, rim, data_dir, batch_size=50)\n",
    "test_dataset = create_dataset(test_meta, rim, test_dir)\n",
    "# train_dataset = load_dataset(data_dir, rim, batch_size=50)\n",
    "# test_dataset = load_dataset(test_dir, rim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(os.path.join(output_dir, \"history.pickle\")):\n",
    "    with open(os.path.join(output_dir, \"history.pickle\"), \"rb\") as f:\n",
    "        history = pickle.load(f)\n",
    "else:\n",
    "    history = {key + \"_train\": [] for key in metrics.keys()}\n",
    "    history.update({key + \"_test\": [] for key in metrics.keys()})\n",
    "    history.update({\"train_loss\": [], \"test_loss\": []})\n",
    "start = time.time()\n",
    "_history = rim.fit(\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    max_time=0.8,\n",
    "    cost_function=cost_function,\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    checkpoints=5,\n",
    "    output_dir=output_dir,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    max_epochs=70,\n",
    "    output_save_mod={\"index_mod\": 300,\n",
    "                     \"epoch_mod\": 1,\n",
    "                     \"step_mod\": 11}, # save first and last step imagees\n",
    "    metrics=metrics,\n",
    "    name=\"rim\"\n",
    ")\n",
    "end = time.time() - start\n",
    "print(f\"Training took {end/60:.02f} minute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(checkpoint_dir, \"hyperparameters.json\"), \"w\") as f:\n",
    "    json.dump(rim.hyperparameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, item in _history.items():\n",
    "    history[key].extend(item)\n",
    "with open(os.path.join(output_dir, \"history.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[\"train_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update of nested dictionaries\n",
    "def update(d, u):\n",
    "    for k, v in u.items():\n",
    "        if isinstance(v, collections.abc.Mapping):\n",
    "            d[k] = update(d.get(k, {}), v)\n",
    "        else:\n",
    "            d[k] = v\n",
    "    return d\n",
    "\n",
    "images = {}\n",
    "for file in glob.glob(os.path.join(output_dir, \"output*\")):\n",
    "    name = os.path.split(file)[-1]\n",
    "    epoch = int(name[7:10])\n",
    "    index = int(name[11:15])\n",
    "    step = int(name[16:18])\n",
    "    with Image.open(file) as image:\n",
    "        im = np.array(image.getdata()).reshape([image.size[0], image.size[1]])\n",
    "        update(images, {index: {epoch : {step: im}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=900\n",
    "images.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = max(images[0].keys())\n",
    "print(epoch)\n",
    "images[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step=max(images[index][epoch])\n",
    "images[index][epoch].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[index][epoch][step], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_file = os.path.join(data_dir, \"image\" + str(index) + \".png\")\n",
    "with Image.open(gt_file) as image:\n",
    "    im = np.array(image.getdata()).reshape([32,32])\n",
    "plt.imshow(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
